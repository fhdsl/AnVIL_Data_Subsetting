[["index.html", "AnVIL Book: Subsetting Your Data with WDL About this Book Skills Level AnVIL Collection", " AnVIL Book: Subsetting Your Data with WDL August 29, 2023 About this Book This book is part of a series of books for the Genomic Data Science Analysis, Visualization, and Informatics Lab-space (AnVIL) of the National Human Genome Research Institute (NHGRI). Learn more about AnVIL by visiting https://anvilproject.org or reading the article in Cell Genomics. Skills Level Genetics Beginner: some genetics knowledge helpful Programming skills Novice: no programming experience needed AnVIL Collection Please check out our full collection of AnVIL and related resources: https://hutchdatascience.org/AnVIL_Collection/ "],["learning-objectives.html", "Learning Objectives", " Learning Objectives Browse the Dockstore repository to find the subsetting workflow Set up the data subsetting workflow Check the workflow output "],["introduction.html", "Chapter 1 Introduction 1.1 The AnVIL Platform 1.2 Subsetting Genomic Data 1.3 Subsetting Data on AnVIL", " Chapter 1 Introduction 1.1 The AnVIL Platform The NHGRI Analysis, Visualization, and Informatics Lab-space (AnVIL) is a web-based platform established to support genomics research and collaboration. AnVIL offers researchers access to a wide array of genomic datasets and computational tools, facilitating the analysis and interpretation of genomics data. The platform’s resources encompass diverse data types, including but not limited to DNA sequences, variants, and functional annotations, enabling researchers to conduct in-depth investigations. AnVIL’s interface provides a range of visualization options for data exploration, and its collaborative features foster interactions among researchers, thereby promoting the exchange of knowledge and expertise. AnVIL serves as the preferred data repository (storage location) for many NIH funded projects. Because it also features many of the analysis tools researchers and clinicians like, such as RStudio, Bioconductor packages, and Galaxy, it can serve as a one-stop-shop. 1.2 Subsetting Genomic Data Genomic data is often on the order of gigabytes for a single sample. This can make many steps of research and clinical pipelines very time and resource intensive! Instead of spending extra time and resources up front, it can be convenient to take subsets of genomic data. Researchers can use software tools, usually via the command line, that take a random subset of reads from large genomic files. Potential applications of subset genomic files include: Performance Testing: Random subsets are useful for evaluating the performance of analysis pipelines or tools. Testing on a smaller dataset allows researchers to assess efficiency, accuracy, and potential issues before applying the pipeline to the entire dataset. Algorithm Development: Researchers creating or fine-tuning algorithms may work with smaller subsets to speed up the development process and debug their code before scaling up to larger datasets. Method Validation: Before conducting a comprehensive analysis, researchers might validate their methods on a smaller subset. This helps ensure the chosen approach is appropriate and produces valid results. Resource Planning: Random subsets provide a representative sample for estimating computational resource requirements, helping researchers allocate sufficient computing power and storage for larger analyses. Pilot Studies: Researchers might use random subsets to perform preliminary investigations, exploring data characteristics and patterns before committing to a full-scale analysis. Teaching and Training: In educational settings, instructors can use manageable subsets for teaching students various analysis techniques without overwhelming them with the complexity of the entire dataset. It also reduces time waiting for analyses to complete. Quick Assessments: Clinicians might use random subsets to quickly assess the presence of specific variants or mutations in patient samples, enabling rapid decision-making for urgent cases. Statistical Sampling: For statistical studies, a random subset can provide representative data for estimating population characteristics without analyzing the entire dataset. Feasibility Studies: Before investing resources in a comprehensive study, researchers might perform feasibility studies on subsets to gauge the potential outcomes and decide whether to proceed. Comparative Analyses: Subsets of different datasets can be easily compared to identify similarities, differences, or trends, aiding in cross-study analyses. Quick Visualizations: Generating visualizations and plots for subsets can offer initial insights and guide researchers in formulating more focused research questions. Publication Preparations: When preparing data for publication, researchers might use subsets to create illustrative figures or example datasets while ensuring data privacy and confidentiality. 1.3 Subsetting Data on AnVIL The AnVIL platform offers a number of advantages compared to institutional servers or other computing resources, such as access to data, integrated tools and pipelines, scalability, and collaboration. Instead of moving data around, users might wish to subset genomic data directly on AnVIL. We have designed a WDL (pronounced “widdle”) workflow that can be run via Terra on AnVIL, subsetting fastq files prior to the next steps in your analysis or activity. Check out the next chapter to learn how to run this workflow on AnVIL! "],["bringing-the-wdl-workflow-to-anvil.html", "Chapter 2 Bringing the WDL Workflow to AnVIL 2.1 Find the Workflow on Dockstore 2.2 Send the Workflow to AnVIL", " Chapter 2 Bringing the WDL Workflow to AnVIL 2.1 Find the Workflow on Dockstore Navigate to the Workflows tab of your Workspace. Select “Find a Workflow”. When the dialog box opens, select “Dockstore” on the bottom left. This will take you to the Dockstore search page. Enter fastq_subsample in the search box. Select the workflow “fhdsl/AnVIL_WDLs/fastq_subsample”. 2.2 Send the Workflow to AnVIL On the right side “Launch with” menu, select “AnVIL”. This will open a new tab. Select the appropriate Workspace where you want the workflow to appear. Select “IMPORT”. You should now be back on AnVIL! "],["set-up-the-workflow.html", "Chapter 3 Set Up the Workflow 3.1 Set Up Inputs 3.2 Set Up Outputs", " Chapter 3 Set Up the Workflow 3.1 Set Up Inputs Use the “SELECT DATA” button to select the samples (rows) where you want to create fastq subsets. You can select all or some samples. Columns of your “DATA” table are used as workflow inputs. You need to specify these. The first workflow input is a fastq or zipped fastq file. The workflow calls this input fastqgz_file_read_1. Under “Attribute” select the column that contains a link to the first set of reads. For single-end sequencing, the fastqgz_file_read_1 input is the only file containing sequencing reads in your data. For paired-end sequencing, the fastqgz_file_read_1 input is the first of two read files. In this example, the column with the fastq file link is called “read1”. It will look like “this.read1” under “Attribute”. Select additional inputs. Required: In this example, we’ve selected “sample_id” as the column containing the name of the sample. This helps with naming the file. Optional: “read2” indicates the second set of reads in our paired-end sequencing approach. Skip this if you have single-end reads. Optional: Indicate how many reads you want in your subsample file. In this example, we wanted 20,000 reads. (Default: 10,000) 3.2 Set Up Outputs Workflow outputs are written to a Google Bucket when run on AnVIL. Setting up the workflow outputs allows us to create links to these outputs inside the DATA table in our Workspace, making them easier to locate. Select the “OUTPUTS” tab. Select “Use defaults” to use the default output column naming schema. Click “SAVE”. "],["run-the-workflow.html", "Chapter 4 Run the Workflow 4.1 Start the Run 4.2 Monitor the Run 4.3 Inspect the Run Results 4.4 Inspecting Other Run Files 4.5 Confirming Results in the DATA tab", " Chapter 4 Run the Workflow 4.1 Start the Run Once you have saved the inputs and outputs, you should be able to click on RUN ANALYSIS. 4.2 Monitor the Run Navigate to the JOB HISTORY tab. You should be able to select your most recent submissions in table form. Click on the most recent submission. Notice how each sample gets its own job, which keeps the whole process speedy! 4.3 Inspect the Run Results Once the runs are completed, you should see the Status change to “Succeeded” if everything ran correctly. After 24 hours, you can also see the costs associated with each run under “Run Cost”. 4.4 Inspecting Other Run Files It can be helpful to look at intermediate files on Google Cloud Platform, especially if runs did not complete successfully. You can view these files by clicking on the folder icon for “Execution directory”. For each run, you can see a number of associated files, including the output .fq files and log files. Click on stdout and/or stderr and “DOWNLOAD” to view the terminal output. Here is what your output in stdout might look like. 4.5 Confirming Results in the DATA tab We can see that the subsample files have been linked in the DATA table “sample”. We had a mixture of single-end and paired-end reads, so only the paired-end samples get a second file. "],["about-the-authors.html", "About the Authors", " About the Authors These credits are based on our course contributors table guidelines.     Credits Names Technical Course Publishing Engineer and Maintainer Ava Hoffman Template Publishing Engineers Candace Savonen, Carrie Wright Publishing Maintenance Engineer Candace Savonen Technical Publishing Stylists Carrie Wright, Candace Savonen Package Developers (ottrpal) John Muschelli, Candace Savonen, Carrie Wright "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
